<!DOCTYPE html>

<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <title>Spoon-Knife</title>
  <LINK href="styles.css" rel="stylesheet" type="text/css">
</head>

<body>

<img src="forkit.gif" id="octocat" alt="" />

<!-- Feel free to change this text here -->
<p>
  Fork me? Fork you, @octocat!
</p>
<p>
  Sean made a change
</p>

<!-- #region 

try:
    response = requests.get(url)
    # ... process the response ...
except requests.exceptions.RequestException as e:
    print(f"An error occurred: {e}")
  
pip install beautifulsoup4 requests
import requests

url = 'YOUR_URL'  # Replace with your URL
response = requests.get(url)

html_content = response.text

from bs4 import BeautifulSoup

soup = BeautifulSoup(html_content, 'html.parser')

target_class = 'YOUR_TARGET_CLASS'  # Replace with your target class
divs = soup.find_all('div', class_=target_class)

for div in divs:
    print(div.get_text())

with open('path/to/your/file.html', 'r') as file:
    html_content = file.read()

  
  
//// USING eval :

  # https://stackoverflow.com/questions/33646365/python-programming-how-to-eval-values-from-dictionary
# https://realpython.com/courses/evaluate-expressions-dynamically-python-eval/

from myfunctions import rank # from myfunctions import rank, mult

#def rank(n):
#    return n/(100.0-0.0)

def mult(n, m): # either define function in the same file OR in a separate file and import eg: see rank() function above
    return n*m

#expr = "dept_id == 'CS1' and student_count > 75 "
#d = {'dept_id': 'CS1' ,'student_count': 95 }
#eval(s, d)
#True
#>>> d = {'dept_id': 'CS1' ,'student_count': 35 }
#>>> eval(s, d)
#False

n=10
m=2
expr = "mult(rank(n), m)"
dict = {'rank': rank ,'mult': mult }

result = eval(expr)
#eval(expr, dict)
print(result)


===========//////////

//==============

  quantstats : 

  qs.plots.snapshot(stock, title='PnL Performance', show=True)

 stock : 
<class 'pandas.core.series.Series'>


Date
2022-09-02   -0.00650
2022-09-06   -0.01258
2022-09-07    0.04119
2022-09-08    0.00484
2022-09-09    0.00319
Name: PnL, dtype: float64

stock.index = DatetimeIndex(['2010-01-04', '2010-01-05', '2010-01-06', '2010-01-07',
               '2010-01-08', '2010-01-11', '2010-01-12', '2010-01-13',
               '2010-01-14', '2010-01-15',
               ...
               '2022-08-26', '2022-08-29', '2022-08-30', '2022-08-31',
               '2022-09-01', '2022-09-02', '2022-09-06', '2022-09-07',
               '2022-09-08', '2022-09-09'],
              dtype='datetime64[ns]', name='Date', length=3168, freq=None)
============ //


  

  
  


  
def ts_arg_max(df, n_days):
    """
    Calculates the number of days since the maximum value in a rolling window of n_days for each column in a DataFrame.

    :param df: DataFrame containing data, with dates as rows and column identifiers.
    :param n_days: Number of days to consider for the rolling window.
    :return: DataFrame with the number of days since the maximum value in each rolling window.
    """
    def days_since_max(column):
        # Reverse the column for rolling to work from past to present
        reversed_col = column.iloc[::-1]
        # Rolling window applied
        return reversed_col.rolling(window=n_days, min_periods=1).apply(lambda x: x.argmax())

    return df.apply(days_since_max)

# Creating a sample DataFrame for testing the ts_arg_max function
sample_data_5 = {
    'Stock1': [10, 12, 14, 15, 14, 13, 12, 11, 10, 9],
    'Stock2': [20, 19, 18, 17, 18, 19, 20, 21, 22, 23],
    'Stock3': [30, 31, 30, 29, 28, 27, 28, 29, 30, 31]
}
sample_df_5 = pd.DataFrame(sample_data_5, index=pd.date_range(start='2021-05-01', periods=10, freq='D'))

# Applying the function to the sample DataFrame with n_days = 3
ts_arg_max_df = ts_arg_max(sample_df_5, n_days=3)
ts_arg_max_df
  
## SIMULATOR VISUALISATION  : https://github.com/ranaroussi/quantstats
#%pip install quantstats

import quantstats as qs

# extend pandas functionality with metrics, etc.
qs.extend_pandas()

# fetch the daily returns for a stock
#port = qs.utils.download_returns('META')
initial_capital = 1e6
stock = pnl['PnL']/initial_capital * 100.0 # stock is extended to be returns stream 

# show sharpe ratio

qs.plots.snapshot(stock, title='PnL Performance', show=True)

qs.reports.html(stock, "SPY")

print(stock.sharpe())
print(stock.max_drawdown())
print(stock.win_rate())
print(stock.avg_win())
print(stock.avg_loss())
print(stock.kelly_criterion())

GENERATE MOCK DATA :
  dates = pd.date_range('2020-01-01', '2020-01-10')
alpha_data = pd.DataFrame(np.random.randn(10, 3), index=dates, columns=['A', 'B', 'C'])
ret_data = pd.DataFrame(np.random.randn(10, 3)/100, index=dates, columns=['A', 'B', 'C'])  # returns in percentage

  ANOTHER WAY GENERATE MOCK DATA :
  # Set random seed for reproducibility
np.random.seed(42)
# Generate a DataFrame with 1000 rows (dates) and 20 columns (stocks)
n_dates = 1000
n_stocks = 50
# Generate random dates
date_rng = pd.date_range(start='1/1/2010', periods=n_dates, freq='D')
# Generate random percentage returns
random_returns = np.random.rand(n_dates, n_stocks) * 0.02 - 0.01
# Create stock names
stock_names = [f'Stock_{i+1}' for i in range(n_stocks)]
# Create the DataFrame
df_returns_random = pd.DataFrame(random_returns, index=date_rng, columns=stock_names)
  
  
## PLOT RANDOM X STOCKS :

  num_random_indices = 10
random_indices = np.random.choice(cp_clean.columns, num_random_indices)
print(random_indices)

# Iterate through the randomly generated column indices
for col_idx in random_indices:
    df_temp = pd.DataFrame()
    df_temp['Close'] = cp_clean.iloc[-100:,int(col_idx)]
    df_temp['Max_fall'] = max_fall_from_last_high(df_temp['Close'])
    #print(df_temp)
    plot_dual_axis(df_temp['Close'], df_temp['Max_fall'])


def winsorize(data, z):
    """
    Clips values in a DataFrame or Series to within 'z' standard deviations of the mean.

    :param data: DataFrame or Series to be winsorized.
    :param z: Number of standard deviations to use for clipping.
    :return: Winsorized DataFrame or Series.
    """
    if isinstance(data, pd.DataFrame):
        # Calculate mean and standard deviation for each column
        mean = data.mean()
        std = data.std()

        # Define clipping bounds
        lower_bound = mean - z * std
        upper_bound = mean + z * std

        # Apply clipping
        winsorized_data = data.clip(lower=lower_bound, upper=upper_bound, axis=1)

    elif isinstance(data, pd.Series):
        # Calculate mean and standard deviation
        mean = data.mean()
        std = data.std()

        # Define clipping bounds
        lower_bound = mean - z * std
        upper_bound = mean + z * std

        # Apply clipping
        winsorized_data = data.clip(lower=lower_bound, upper=upper_bound)

    else:
        raise TypeError("Input data must be a pandas DataFrame or Series")

    return winsorized_data

# Example usage:
# For a DataFrame 'df'
# winsorized_df = winsorize(df, z=2)

# For a Series 'series'
# winsorized_series = winsorize(series, z=2)

def neutralize(df, group, include_itself=True, min_group_size=2):
    """
    Neutralizes each value in 'df' by subtracting the mean of its respective group as defined in 'group'.
    Groups with fewer than min_group_size stocks are excluded from the neutralization process.

    :param df: DataFrame containing stock prices, with dates as rows and stocks as columns.
    :param group: Series containing group IDs for each stock.
    :param include_itself: Boolean indicating whether to include the stock's own value in the mean calculation.
    :param min_group_size: Minimum number of stocks in a group to be included in neutralization.
    :return: Neutralized DataFrame.
    """
    # Ensure the group series is aligned with the columns of df
    group = group.reindex(df.columns)

    # Count the number of stocks in each group
    group_counts = df.groupby(group, axis=1).count()

    # Calculate group means with the specified method
    if include_itself:
        group_means = df.groupby(group, axis=1).transform('mean')
    else:
        group_means = (df.groupby(group, axis=1).transform('sum') - df) / (df.groupby(group, axis=1).transform('count') - 1)

    # Apply neutralization only to groups with a sufficient number of stocks
    neutralized_df = df.sub(group_means).where(group_counts >= min_group_size, df)

    return neutralized_df

# Example usage:
# neutralized_df = neutralize(df, group, include_itself=True, min_group_size=2)
  
  # ALSO set all exact 0 to NAN,
def neutralize(df, group, include_itself=True):
    """
    Neutralizes each value in 'df' by subtracting the mean of its respective group as defined in 'group'.
    If include_itself is True, includes the stock's own value in the mean calculation. If False, excludes it.

    :param df: DataFrame containing stock prices, with dates as rows and stocks as columns.
    :param group: Series containing group IDs for each stock.
    :param include_itself: Boolean indicating whether to include the stock's own value in the mean calculation.
    :return: Neutralized DataFrame.
    """
    # Ensure the group series is aligned with the columns of df
    group = group.reindex(df.columns)

    if include_itself:
        # Calculate group means for each date including the stock itself
        group_means = df.groupby(group, axis=1).transform('mean')
    else:
        # Calculate group means for each date excluding the stock itself
        group_means = (df.groupby(group, axis=1).transform('sum') - df) / (df.groupby(group, axis=1).transform('count') - 1)

    # Subtract the group mean from each stock's value
    neutralized_df = df.sub(group_means)

    return neutralized_df

# Example usage:
# Assuming df is your DataFrame of stock prices and group is the Series of group IDs
# neutralized_df_included = neutralize(df, group, include_itself=True)
# neutralized_df_excluded = neutralize(df, group, include_itself=False)
# print(neutralized_df_included)
# print(neutralized_df_excluded)


## ANALYZING TURNOVER :
  ## ANALYZING ALPHA : TURNOVER

from scipy.stats import linregress

def calculate_turnover(alpha):
    alpha_df = pd.DataFrame(alpha)
    turnover = alpha_df.diff().abs().sum(axis=1) / 2
    return turnover

alpha = np.random.rand(*ret1_d1.shape)
alpha = alpha / alpha.sum(axis=1)[:, np.newaxis]

returns_alpha = np.sum(alpha * ret1_d1.fillna(0).values, axis=1)
returns_series = pd.Series(returns_alpha)

## ANALYZING TURNOVER :
# 1. Correlate Turnover with Returns: Check if higher turnover periods are associated with higher returns. If not, the high turnover might just be noise. In this case, can use smoothing or regularization or longer holding periods or batch trading(aggregate trades first)
# 2. Transaction Costs: High turnover means higher transaction costs. Make sure that the extra returns you're getting are not being eaten up by these costs.
# 3. Consistency: If the alpha is consistently profitable but has high turnover, you might be able to reduce the turnover without sacrificing profitability by applying some form of regularization or smoothing to your alpha construction logic.
# 4. Review Alpha Factors: Carefully review the alpha factors you're using. Are they all contributing positively to your returns? Removing unnecessary factors can reduce turnover. 

turnover = calculate_turnover(alpha)
correlation_with_returns = turnover.corr(returns_series)


## FUNCTIONS 

  def ts_max(df, window):
    df = df.rolling(window).max()
    return df
    
def ts_arg_max(df, window):
    ## this function calculates the index of the max value in the rolling window, this index is the actual index in the dataframe
    def idx_max(x):
        dict_values = [x.shift(i) for i in range(window)]
        df_shift = pd.DataFrame(dict(zip(range(window),dict_values))).fillna(0)
        return x.index[np.arange(len(x)) - df_shift.idxmax(1)]
    return df.apply(lambda x: idx_max(x))
    
def group_rank(df, group):
    ## implement this function, this function will apply a custom function, 'rank' in this case(.apply() above rank function) , group-wise where group is sector here. So instead of applying rank to whole row, it will only rank the stock among it's sector stocks
    df_merge = pd.merge(df.T, group.replace(np.nan,'NAN').T, right_index = True, left_index = True)
    df = df.apply(lambda x: df_merge.groupby(x.name+'_y',)[x.name+'_x'].rank(), axis = 1)
    return df
    
def bucket_rank(df, n_bucket):
    ## implement this function , 
    ## this takes in a df_bucket dataframe
    ##    For the df_bucket, first convert each row to uniform values between [0,1] using min-max scaling ie (value - min(value across the row))/(max-min)
    ##    Now divide the ranged bucket values to n_bucket, eg : if n_bucket=5, then df_bucket=0-0.2 will be bucket1, 0.2-0.4 will be bucket2 and so on...
    ##    Now we have bucket number for each stock for each dasy just like sector, this is the new classification
    
    ## Now apply rank to this new group 'bucket' , apply rank like a customized functionreturn because I would like to change it to other functions like neutralizeation etc as well
    ## Make the code easy to understand and extendible by me
    
    groups = [f'bucket{i}' for i in range(1, n_bucket+1)]
    edges = np.linspace(0,1,num = n_bucket+1)
    df_group = df.apply(lambda row: pd.cut((row - row.min())/(row.max()-row.min()), bins = edges, labels = groups, duplicates='drop').astype('O')) 
    return group_rank(df,df_group)

  

  
  # eutralizes group which is a series adn df is proper
def neutralize(df, group):
    """
    Neutralizes each value in 'df' by subtracting the mean of its respective group as defined in 'group'.

    :param df: DataFrame containing stock prices, with dates as rows and stocks as columns.
    :param group: Series containing group IDs for each stock.
    :return: Neutralized DataFrame.
    """
    # Ensure the group series is aligned with the columns of df
    group = group.reindex(df.columns)

    # Calculate group means for each date
    group_means = df.groupby(group, axis=1).transform('mean')

    # Subtract the group mean from each stock's value
    neutralized_df = df.sub(group_means)

    return neutralized_df

  
def truncate_new(df, fraction):
    """
    Truncates each value in a DataFrame based on a specified fraction of the absolute sum of values in that row.

    :param df: DataFrame to be truncated.
    :param fraction: The fraction of the total absolute sum to limit each value to.
    :return: Truncated DataFrame.
    """
    # Calculate the total absolute sum per row
    total_abs_sum = df.abs().sum(axis=1)

    # Apply conditional truncation
    truncated_df = df.apply(lambda row: np.where(row > fraction * total_abs_sum[row.name], 
                                                 fraction * total_abs_sum[row.name], 
                                                 np.where(row < -fraction * total_abs_sum[row.name], 
                                                          -fraction * total_abs_sum[row.name], 
                                                          row)), axis=1)

    return pd.DataFrame(truncated_df.tolist(), index=df.index, columns=df.columns)

# Applying the new truncate function to the example DataFrame
truncated_df_new = truncate_new(df_example, 0.02)
truncated_df_new


  
def group_mean(df, group, include_itself=True):
    """
    Calculates the mean of all values in the group of a particular stock.

    :param df: DataFrame containing stock prices, with dates as rows and stocks as columns.
    :param group: DataFrame containing industry IDs for each stock, with same structure as df.
    :param include_itself: If True, include the stock's own value in mean calculation. If False, exclude it.
    :return: DataFrame of group means for each stock.
    """
    # Check if df and group have the same structure
    if df.shape != group.shape or (df.index != group.index).any() or (df.columns != group.columns).any():
        raise ValueError("df and group must have the same shape, indices, and columns")

    # Create a copy of df for modification
    group_means = df.copy()

    # Iterate over each date and stock
    for date in df.index:
        for stock in df.columns:
            group_id = group.at[date, stock]
            group_values = df.loc[date][group.loc[date] == group_id]

            if not include_itself:
                group_values = group_values[group_values.index != stock]

            group_mean_value = group_values.mean()
            group_means.at[date, stock] = group_mean_value

    return group_means

# Testing the group_mean function with the same example data
group_means_included = group_mean(df_example, group_example, include_itself=True)
group_means_excluded = group_mean(df_example, group_example, include_itself=False)

group_means_included, group_means_excluded


def neutralize(df, group, include_itself=True):
    """
    Neutralizes each value in 'df' by subtracting the mean of all stocks in the same 'group'.

    :param df: DataFrame containing stock prices, with dates as rows and stocks as columns.
    :param group: DataFrame containing industry IDs for each stock, with same structure as df.
    :param include_itself: If True, include the stock's own value in mean calculation. If False, exclude it.
    :return: Neutralized DataFrame.
    """
    # First, calculate the group means
    group_means = group_mean(df, group, include_itself)

    # Neutralize by subtracting the group mean from each stock's value
    neutralized_df = df.sub(group_means)

    return neutralized_df

# Testing the neutralize function with the same example data
neutralized_df_included = neutralize(df_example, group_example, include_itself=True)
neutralized_df_excluded = neutralize(df_example, group_example, include_itself=False)

neutralized_df_included, neutralized_df_excluded

def neutralize(df, group):
    """
    Neutralizes each value in 'df' by subtracting the mean of all stocks in the same 'group'.

    :param df: DataFrame containing stock prices, with dates as rows and stocks as columns.
    :param group: DataFrame containing industry IDs for each stock, with same structure as df.
    :return: Neutralized DataFrame.
    """
    # Ensuring the indices and columns of df and group match
    if df.shape != group.shape or (df.index != group.index).any() or (df.columns != group.columns).any():
        raise ValueError("df and group must have the same shape, indices, and columns")

    # Applying the neutralization
    return df.sub(df.groupby(group).transform('mean'))
  
def find_missing_values_NAN(df):

    '''
        This function finds out missing values (as NAN) in between valid non-NAN values
    '''
    
    print("Missing values as NAN : \n")
    
    found=False
    for i in range(1, len(df) - 1):  # Exclude first and last row
        for j in range(1, len(df.columns) - 1):  # Exclude first and last column
            if pd.isna(df.iloc[i, j]) and not pd.isna(df.iloc[i-1, j]) and not pd.isna(df.iloc[i+1, j]):
                print(f"NaN value surrounded by non-NaN values at (row {i}, column {j})\n")
                print(df.iloc[i-2:i+2,j])
                found=True
                break
            if found:
                break
                
def find_missing_values_zero(df):
    
    '''
        This function finds out missing values (as 0) in between valid non-NAN values
    '''
    
    print("Missing values as 0 : \n")
    
    found=False
    for i in range(1, len(df) - 1):  # Exclude first and last row
        for j in range(1, len(df.columns) - 1):  # Exclude first and last column
            if df.iloc[i, j] == 0 and df.iloc[i-1, j] != 0 and df.iloc[i+1, j] != 0:
                print(f"zero value surrounded by non-zero values at (row {i}, column {j})\n")
                print(df.iloc[i-2:i+2,j])
                found=True
                break
            if found:
                break

def find_inf_values(df):
    
    '''
        This function find inf values in the data
    '''
    
    print("Check for inf values : \n")
    
    found=False
    for i in range(1, len(df) - 1):  # Exclude first and last row
        for j in range(1, len(df.columns) - 1):  # Exclude first and last column
            if np.isinf(df.iloc[i, j]) and not np.isinf(df.iloc[i-1, j]) and not np.isinf(df.iloc[i+1, j]):
                print(f"inf value surrounded by non-inf values at (row {i}, column {j})\n")
                print(df.iloc[i-2:i+2,j])
                found=True
                break
            if found:
                break
                
def find_non_NAN_rows(df):
    non_nan_rows = df[df.apply(lambda row: row.notna().any(), axis=1)]
    print(non_nan_rows.head(10))

def plot_histogram_whole_data(df, range_boundary):

    fig, ax = plt.subplots()  # Create a new figure and axes for each plot
    
    
    df = df.stack()
    df = df.reset_index(drop=True)
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.dropna(axis=0, how='any', inplace=True)
    #splt.hist(df,bins=20)
    #plt.hist(df, bins=20, range=(-1*range_boundary, range_boundary))
    
    ax.hist(df, bins=20, range=(-1 * range_boundary, range_boundary))
    
# cross-section plotting
def plot_cs(df, row_index, x_title, bins=20):
    
    fig, ax = plt.subplots()  # Create a new figure and axes for each plot
    
    df = df.iloc[row_index, :]
    #plt.hist(df, bins=20)  
    ax.hist(df, bins=20)
    plt.xlabel(x_title)
    plt.ylabel('Frequency')
    plt.title('Cross-Sectional Plot : Histogram of '+str(x_title))
    plt.show();

# time series plottimg
def plot_ts(df, column_index, y_title):

    
    #if remove_nan:
    #    df = df.iloc[:,column_index].dropna()
   
    plt.plot(df.index, df.iloc[:, column_index])
    plt.xlabel('Date')
    plt.ylabel(y_title)
    plt.title('Time Series Plot : '+str(y_title)+' for Column {}'.format(column_index))
    plt.show();

def plot_ts_df1_vs_df2(df1, df2, column_index):
    """
    Plot time series for two given dataframes for a specified column.
    
    Parameters:
    df1 (DataFrame): DataFrame containing first set of data with dates as index.
    df2 (DataFrame): DataFrame containing second set of data with dates as index.
    column_index (int or str): Column representing the stock in both DataFrames.
    
    Returns:
    None: Displays the plot.
    """
    
    # Convert column_index to string to match DataFrame's column type
    column_index = str(column_index)
    
    # Get DataFrame names for labels
    df1_name = df1.name if hasattr(df1, 'name') else 'df1'
    df2_name = df2.name if hasattr(df2, 'name') else 'df2'
    
    fig, ax1 = plt.subplots(figsize=(15, 8))
    
    # Plot data from df1
    ax1.set_xlabel('Date')
    ax1.set_ylabel(df1_name, color='tab:blue')
    ax1.plot(df1.index, df1[column_index], color='tab:blue', label=df1_name)
    ax1.tick_params(axis='y', labelcolor='tab:blue')
    
    # Create a secondary y-axis to plot data from df2
    ax2 = ax1.twinx()
    ax2.set_ylabel(df2_name, color='tab:red')
    ax2.plot(df2.index, df2[column_index], color='tab:red', label=df2_name)
    ax2.tick_params(axis='y', labelcolor='tab:red')
    
    # Show legend and grid
    ax1.legend(loc='upper left')
    ax2.legend(loc='upper right')
    ax1.grid(True)
    
    plt.title(f'Time Series for {df1_name} and {df2_name} for Column {column_index}')
    plt.show()

    
    
def plot_ts_random(df, n=4):
    """
    Plot n random time series columns from a given DataFrame df as subplots in a 2x(n/2) table format.
    
    Parameters:
    df (DataFrame): DataFrame containing the time series data.
    n (int): Number of random columns to plot. Default is 4, should be an even number.
    
    Returns:
    None: Displays the subplots.
    """
    
    # Ensure n is an even number for 2x(n/2) table format
    if n % 2 != 0:
        n = int(n/2)*2
    
    # Get DataFrame name for labeling purposes
    df_name = df.name if hasattr(df, 'name') else 'DataFrame'
    
    # Select n random columns from the DataFrame
    random_columns = random.sample(list(df.columns), n)
    
    # Create subplots in 2x(n/2) table format
    fig, axs = plt.subplots(int(n/2), 2, figsize=(15, 4 * int(n/2)))
    
    for i, column in enumerate(random_columns):
        row = i // 2
        col = i % 2
        axs[row, col].plot(df.index, df[column])
        axs[row, col].set_xlabel('Date')
        axs[row, col].set_ylabel(df_name)
        axs[row, col].set_title(f'Column {column}')
    
    plt.tight_layout()
    plt.show()
    
'''
# display histogram of % of non-NAN values for each column
def plot_data_completeness(df):
    
    non_nan_percentage = df.notna().mean() * 100
    plt.figure(figsize=(10, 6))
    
    plt.hist(non_nan_percentage, bins=50, edgecolor='black')
    plt.title('Histogram of Percentage of Non-NaN Entries Per Column')
    plt.xlabel('Percentage of Non-NaN Entries')
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.show()
    return non_nan_percentage
'''

# display histogram of % of non-NAN values for each column
# trim_leading_NANs True means you compute the calculation after removing leading NANs
def plot_data_completeness(df, trim_leading_NANs=False):
    if trim_leading_NANs:
        # Initialize an empty dictionary to store the non-NaN percentages after trimming leading NaNs
        non_nan_percentage = {}
        for col in df.columns:
            # Drop leading NaNs
            trimmed_series = df[col].dropna()
            if len(trimmed_series) > 0:
                first_valid_index = df[col].first_valid_index()
                trimmed_series = df.loc[first_valid_index:, col]
                non_nan_percent = (trimmed_series.count() / len(trimmed_series)) * 100
            else:
                non_nan_percent = 0.0
            non_nan_percentage[col] = non_nan_percent
        non_nan_percentage = pd.Series(non_nan_percentage)
    else:
        non_nan_percentage = df.notna().mean() * 100
    
    plt.figure(figsize=(10, 6))
    plt.hist(non_nan_percentage, bins=50, edgecolor='black')
    plt.title('Histogram of Percentage of Non-NaN Entries Per Column')
    plt.xlabel('Percentage of Non-NaN Entries')
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.show()
    
    return non_nan_percentage

def normalize(df):
    df = df.sub(df.mean(axis=1), axis=0)
    df = df.div(df.abs().sum(axis=1), axis=0)
    return df
    
def neutralize(df):
    df = df.sub(df.mean(axis=1), axis=0)
    return df

##def neutralized_with_sector(df):
    ### IMPLEMENT? ?
    
def rank(df):
    df = df.apply(lambda row: row.rank(), axis=1)
    df = df.div(df.sum(axis=1), axis=0)
    return df

def ts_rank(df, n):
    #df = df.squeeze()
    # rolling applies to series
    df = df.rolling(window=n).apply(rank)
      
def group_mean(df, group):

    '''
    Returns data with average value of returns based on sector for each day and stock
    '''
    # The function uses 2 dictionaries; 1st to get sectors from sector data as keys and mean of returns for each sector for a day as values
    # 2nd dict contains date as keys and 1st dictionary as value for correcsponding date
    # here function uses Transpose of data to convert dates in columns 
    # as a last step function replaces sectors in sector data with mean values for each column and again applies Transpose to get original form

    keys = []
    values = []
    for col in group.T.columns:
        daywise_df = pd.DataFrame({'sector':group.T[col],'returns':df.T[col]}) # data of sector info and stocks' returns on particular day
        dict_sector_mean = dict(daywise_df.groupby('sector')['returns'].mean()) # dict of sector with corresponding mean value for a particular day
        values.append(dict_sector_mean)
        keys.append(col)
    return group.T.replace(dict(zip(keys,values))).T # Replace sector with corresponding means
    
def neutralize_to_group(df, group):
    df = df.sub(group_mean(df, group))
    return df

def replace_outliers_with_nan(df):
    '''
        This function replaces values which are higher than 99th percentile or below than 1 percentile by NAN
    '''
    q99 = df.quantile(0.99)
    q1 = df.quantile(0.01)
    
    df[df > q99] = np.nan
    df[df < q1] = np.nan
    
    return df

def calculate_cvar(returns, alpha=0.05):
    """
    Calculate the Conditional Value-at-Risk (CVaR) of a series of returns.
    
    Parameters:
    - returns (numpy.array): Array of returns
    - alpha (float): Significance level, default is 0.05
    
    Returns:
    - float: CVaR value
    """
    # Sort the returns
    sorted_returns = np.sort(returns)
    
    # Calculate the index corresponding to the alpha quantile
    index = int(alpha * len(sorted_returns))
    
    # Calculate CVaR
    cvar = np.mean(sorted_returns[:index])
    
    return cvar

Kalman filter smoothening :

def kalman_filter_smoothing_df(df, observation_covariance, transition_covariance):
    n_rows, n_cols = df.shape
    smoothed_df = pd.DataFrame(index=df.index, columns=df.columns)

    for col in df.columns:
        series = df[col]
        initial_state = series.dropna().iloc[0] if not series.dropna().empty else 0
        state = initial_state
        state_covariance = 1.0
        filtered_state_means = np.zeros(n_rows)

        for t in range(n_rows):
            # Prediction Step
            predicted_state = state
            predicted_state_covariance = state_covariance + transition_covariance

            # Update Step
            observation = series.iloc[t] if not np.isnan(series.iloc[t]) else predicted_state  # Use prediction if observation is NaN
            kalman_gain = predicted_state_covariance / (predicted_state_covariance + observation_covariance)
            state = predicted_state + kalman_gain * (observation - predicted_state)
            state_covariance = (1 - kalman_gain) * predicted_state_covariance

            filtered_state_means[t] = state

        smoothed_df[col] = filtered_state_means

    return smoothed_df

# Testing the adapted function on the first two columns of the data
observation_covariance_test = 0.5
transition_covariance_test = 0.05
smoothed_data_test = kalman_filter_smoothing_df(cp.iloc[:, :7], observation_covariance_test, transition_covariance_test)
print(smoothed_data_test.head(5))

# plot 7th column
plt.figure(figsize=(14, 6))
plt.plot(cp.iloc[:,6], label=f'Original', color='blue', linewidth=2)
plt.plot(smoothed_data_test.iloc[:,6], label=f'Kalman Smoothed (ObsCov={observation_covariance}, TransCov={transition_covariance})', color='green', linewidth=2)            
plt.title(f'Stock Price: Kalman Filter vs Original')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

def plot_series_histogram(series):
    """
    Plots the histogram of a pandas Series.

    :param series: Pandas Series to be plotted.
    """
    # Plotting the histogram
    plt.hist(series, bins='auto', color='blue', alpha=0.7)
    plt.title('Histogram of Series')
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    plt.show()
-->

</body>
</html>
