<!DOCTYPE html>

<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <title>Spoon-Knife</title>
  <LINK href="styles.css" rel="stylesheet" type="text/css">
</head>

<body>

<img src="forkit.gif" id="octocat" alt="" />

<!-- Feel free to change this text here -->
<p>
  Fork me? Fork you, @octocat!
</p>
<p>
  Sean made a change
</p>

<!-- #region 


  EVENT SIGNAL :
import pandas as pd

# Assuming 'close' is your DataFrame with stocks along columns and dates as rows


  # Convert data to float32 for efficiency
close = close.astype('float32')
  
# 1. Calculate daily returns
daily_returns = close.pct_change()

# 2. Rolling window to find the day of highest return in the past 20 days
rolling_max = daily_returns.rolling(window=20).idxmax()

# 3. Calculate the number of days since the day of highest return
current_date = close.index[-1]  # Assuming you want the signal for the current date
days_since_max = (current_date - rolling_max).dt.days

# 4. Compute the signal
signal = rolling_max.apply(lambda x: x.day) / (1 + days_since_max)

# 'signal' now contains the computed signals for each stock



## OPTIMIZE MEMORY 
python -m memory_profiler sample_script.py
pip install memory_profiler

from memory_profiler import profile

class Signal:
    @profile
    def on_signal(self, ...):
        # method implementation

    @profile
    def on_starting(self, ...):
        # method implementation

  ## OPTIMIZE SPEED
python -m cProfile sample_script.py

import cProfile
# Assuming run_backtest is your backtesting function
cProfile.run('run_backtest()', 'profiling_output.prof')

snakeviz profiling_output.prof

######OPTIMIZE #######


CREATE SAMPLE DATA for optimize_position :
import pandas as pd
import numpy as np

# Set a seed for reproducibility
np.random.seed(0)

# Sample data for df_pos (positions of stocks)
# Assuming these are normalized positions (summing to 1 for simplicity, but they can be any values)
df_pos = pd.Series(np.random.rand(10), name='Position')

# Sample data for df_group (industry numbers for the stocks)
# Let's assume 4 different industries, denoted by numbers 1 to 4
df_group = pd.Series(np.random.randint(1, 5, 10), name='Industry')

# Set stock names as index (for example, Stock1, Stock2, ..., Stock10)
df_pos.index = [f'Stock{i+1}' for i in range(10)]
df_group.index = df_pos.index

# Display the generated data
print("Positions (df_pos):")
print(df_pos)
print("\nIndustry Numbers (df_group):")
print(df_group)
  

  import cvxpy as cp
import numpy as np
import pandas as pd

  
    # Revised group constraints
    for g in groups:
        group_mask = (df_group == g).astype(float).values  # Create a mask for each group
        group_sum = cp.sum(cp.multiply(x, group_mask))  # Elementwise multiplication
        constraints += [group_sum <= 0.02 * total_abs_sum, 
                        group_sum >= -0.02 * total_abs_sum]

  
def optimize_portfolio(df_pos, df_group, initial_guess=None):
    """
    Optimize the portfolio positions with an option for warm start.

    Args:
    df_pos (pd.Series): Initial positions of various stocks.
    df_group (pd.Series): Industry numbers for the stocks.
    initial_guess (np.array, optional): Initial guess for the solution.

    Returns:
    pd.Series: Optimized positions in the same format as df_pos.
    """

    if not isinstance(df_pos, pd.Series) or not isinstance(df_group, pd.Series):
        raise ValueError("df_pos and df_group must be pandas Series")
    if not df_pos.index.equals(df_group.index):
        raise ValueError("Indices of df_pos and df_group must match")

    n = len(df_pos)
    groups = df_group.unique()

    x = cp.Variable(n)
    objective = cp.Minimize(cp.sum_squares(x - df_pos.values))

    constraints = [cp.sum(x) == 0]
    total_abs_sum = cp.sum(cp.abs(x))
    constraints += [x >= -0.01 * total_abs_sum, x <= 0.01 * total_abs_sum]

    for g in groups:
        group_indices = df_pos.index[df_group == g]
        constraints += [cp.sum(x[group_indices]) <= 0.02 * total_abs_sum, 
                        cp.sum(x[group_indices]) >= -0.02 * total_abs_sum]

    prob = cp.Problem(objective, constraints)

    # Warm start
    if initial_guess is not None:
        if len(initial_guess) != n:
            raise ValueError("Initial guess must have the same length as df_pos")
        x.value = initial_guess

    # Solve the problem using the OSQP solver
    prob.solve(solver=cp.OSQP) # ECOS/SCS/OSQP

    optimized_positions = pd.Series(x.value, index=df_pos.index)
    return optimized_positions

# Example usage:
# initial_guess = np.zeros(len(df_pos))  # or some other initial guess (last day's values)
# optimized_positions = optimize_portfolio(df_pos, df_group, initial_guess)

## add barra exposure condition :

Normalize Factor Loadings: Normalize the factor loadings for each stock. This can be done by dividing each factor loading by the sum of the absolute values of the loadings for that factor across all stocks. This normalization ensures that the constraint considers the relative importance of each stock's exposure to a factor.

def optimize_portfolio(df_pos, df_group, df_factors, initial_guess=None):
    """
    [Existing docstring]

    Modified to include normalized factor loading constraints.
    """
    
    # [Existing checks and setup code]

    # Normalize factor loadings
    factor_loading_sums = df_factors.abs().sum()
    normalized_factors = df_factors.divide(factor_loading_sums, axis=1)
    
    # Constraint: Limiting factor exposures with normalized factor loadings
    F = normalized_factors.values  # Normalized factor loading matrix
    total_abs_sum = cp.sum(cp.abs(x))

    for j in range(F.shape[1]):  # Loop over each factor
        factor_exposure = cp.sum(cp.multiply(F[:, j], x))
        constraints += [cp.abs(factor_exposure) <= 0.01 * total_abs_sum]

    # [Rest of your existing code to set up and solve the optimization problem]

    return pd.Series(x.value, index=df_pos.index)

  


  
# visualize the optimized positions :

import matplotlib.pyplot as plt

# Assuming you have run the optimization and have `df_pos` and `optimized_positions`
plt.figure(figsize=(12, 6))
plt.plot(df_pos, label='Initial Positions', marker='o')
plt.plot(optimized_positions, label='Optimized Positions', marker='x')
plt.title('Portfolio Optimization Comparison')
plt.xlabel('Stocks')
plt.ylabel('Positions')
plt.xticks(rotation=45)
plt.legend()
plt.show()

Plot pnl vs position size on the day and its industry and industry pnl on that day , across various groups ??
  
#######################

  

import numpy as np

def gram_schmidt_orthogonalize(alpha, pca_matrix):
    """
    Orthogonalize a given vector 'alpha' using the Gram-Schmidt process against a set of vectors (PCA matrix).
    
    Parameters:
    - alpha (numpy.array): The vector to be orthogonalized
    - pca_matrix (numpy.array): The matrix containing the vectors to orthogonalize against (PCA components)
    
    Returns:
    - numpy.array: The orthogonalized vector
    """
    
    # Initialize the orthogonalized vector as the original alpha
    orthogonalized_alpha = np.array(alpha, dtype=float)
    
    # Iterate through each vector in the PCA matrix to orthogonalize
    for i in range(pca_matrix.shape[1]):
        pca_vector = pca_matrix[:, i]
        
        # Compute the projection of alpha onto the PCA vector
        projection = np.dot(alpha, pca_vector) / np.dot(pca_vector, pca_vector)
        
        # Subtract the projection from the orthogonalized vector
        orthogonalized_alpha -= projection * pca_vector
    
    return orthogonalized_alpha

# Test the function with a sample alpha vector (replace this with your actual alpha)
sample_alpha = np.random.rand(pca_result.shape[0])

# Orthogonalize the sample alpha vector against the PCA components
orthogonalized_alpha = gram_schmidt_orthogonalize(sample_alpha, pca_result)



2. -> 
# OPTIMIZE FRAMEWORK

import matplotlib.pyplot as plt
import numpy as np
from scipy.optimize import minimize

def optimizer(func_to_optimize, initial_guess, bounds):
    evaluated_values = []  # Stores evaluated function values for plotting
    
    def evaluate(params):
        
        ## use the below to step paramater values ie 0:2:20 
        #params[0] = round(params[0] / 2) * 2  # Round to nearest multiple of 2, use this tactic, if you want to run parameter 1 at a step of 2 ie bounds=(0,20) and with this line, it will be 0,2,4,...20
        #params[0] = round(params[0] / 0.5) * 0.5  # Round to nearest multiple of 0.5, this is same as above but will have step of 0.5, so bounds=(0,20) will be 0,0.5,1,1.5,...10

        # for categorical params, use dictionary
        #string_options = ["sector", "industry", "subindustry"]
        #params[0] = string_options[int(round(params[0]))]  # Round to nearest integer and map to string value
        
        value = -func_to_optimize(*params)
        evaluated_values.append(value)
        
        return value
    
    result = minimize(evaluate, initial_guess, bounds=bounds)
    
    if result.success:
        optimal_params = result.x
        max_value = -result.fun
    else:
        optimal_params = None
        max_value = None
        print("Optimization failed:", result.message)
    
    # Real-time plotting
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(evaluated_values, marker='o')
    plt.xlabel('Iteration')
    plt.ylabel('Function Value')
    plt.title('Function Value by Iteration')
    
    # Parameter-wise plotting
    for i, bound in enumerate(bounds):
        param_values = np.linspace(bound[0], bound[1], 20)
        func_values = []
        
        for val in param_values:
            temp_params = optimal_params.copy()
            temp_params[i] = val
            func_values.append(-evaluate(temp_params))
        
        plt.subplot(1, 2, 2)
        plt.plot(param_values, func_values, marker='o', label=f'Param {i+1}')
        
    plt.xlabel('Parameter Value')
    plt.ylabel('Function Value')
    plt.title('Function Value by Parameter values (keeping other parameter at their optimized values)')
    plt.legend()
    plt.tight_layout()
    plt.show()
    
    return optimal_params, max_value

# Example usage
if __name__ == "__main__":
    
    # CHANGE THIS FUNCTION
    def example_function(x, y, z):
        return -((x-1)**2 + (y+2)**2 + z) 
    
    initial_guess = [0, 0, 0]
    # bounds = [(0, len(string_options) - 1)]  # Parameter bounds: index is between 0 and len(string_options) - 1 ; USE this when using categorical params string_options
    bounds = [(-3, 3), (-3, 3), (-5,5)]
    
    '''
    bounds_x = np.arange(0, 21, 2)  # Use np.arange to create an array from 0 to 20 with step 2
    bounds_y = np.arange(0, 11, 0.5)  # Use np.arange to create an array from 0 to 10 with step 0.5
    string_options = ["sector", "industry", "subindustry"] # categorical variables
    bounds_category = (0, len(string_options) - 1)  # Parameter bounds: index is between 0 and len(string_options) - 1
    bounds = [bounds_x, bounds_y, bounds_category]
    '''
    
    optimal_params, max_value = optimizer(example_function, initial_guess, bounds)
    print("Optimal parameters:", optimal_params)
    print("Maximum value:", max_value)


3. ->

# Function to plot returns distribution with respect to adv_ratio_d1
def plot_returns_vs_feature(returns, feature, num_buckets=10):
    # Stack both dataframes for easier manipulation
    stacked_returns = returns.stack().reset_index().rename(columns={0: 'Returns', 'level_1': 'Stock'})
    stacked_feature = feature.stack().reset_index().rename(columns={0: 'Feature', 'level_1': 'Stock'})

    # Remove instances where returns are zero
    stacked_returns = stacked_returns[stacked_returns['Returns'] != 0]
    
    # Merge them together
    merged_data = pd.merge(stacked_returns, stacked_feature, on=['level_0', 'Stock'])
    
    # Create adv_ratio buckets
    merged_data['Feature_Bucket'] = pd.qcut(merged_data['Feature'], q=num_buckets, labels=[f'Bucket_{i+1}' for i in range(num_buckets)])

    # Calculate statistics for each bucket
    bucket_stats = merged_data.groupby('Feature_Bucket')['Returns'].agg(['mean', 'min', 'max']).reset_index()
    
    # Plotting
    plt.figure(figsize=(12, 6))
    plt.plot(bucket_stats['Feature_Bucket'], bucket_stats['mean'], marker='o', label='Mean Returns')
    plt.plot(bucket_stats['Feature_Bucket'], bucket_stats['min'], marker='x', label='Min Returns')
    plt.plot(bucket_stats['Feature_Bucket'], bucket_stats['max'], marker='s', label='Max Returns')
    plt.xlabel('Feature_Buckets')
    plt.ylabel('Returns')
    plt.title('Returns vs Feature')
    plt.legend()
    plt.grid(True)
    plt.show()
=========

  
import os

def parse_and_write(file_path):
    with open(file_path, 'r') as file:
        lines = file.readlines()

    current_folder = None

    for line in lines:
        line = line.strip()
        if not line:  # Skip empty lines
            continue

        # Check if the line is a header
        if line.startswith('IDEA:'):
            current_folder = line.split(':')[1]
            if not os.path.exists(current_folder):
                os.makedirs(current_folder)
        else:
            # Split the expression name and expression
            parts = line.split(',', 1)
            if len(parts) == 2:
                expression_name, expression = parts
                subfolder_path = os.path.join(current_folder, expression_name.strip())
                if not os.path.exists(subfolder_path):
                    os.makedirs(subfolder_path)

                with open(os.path.join(subfolder_path, 'data.txt'), 'w') as data_file:
                    data_file.write(expression.strip())

# Usage
parse_and_write('expression.txt')

  import os
file_exists = os.path.exists(os.path.join('folder_path', 'filename.DONE'))


import re

def get_content(variable_name):
    # Your existing function to get content based on the variable name
    # For demonstration, let's just return a dummy value
    return [1, 2, 3, 4, 5]  # Replace with your actual content retrieval logic

def assign_variables(expr):
    # Find all occurrences of words followed by an opening parenthesis
    #variables = re.findall(r'\b\w+(?=\()', expr)
    variables = re.findall(r'\b[A-Z][A-Z_\d]*\b', expr) # starts with upper case ltter, can have underscores, upper case letters and digits

    for var in variables:
        # Dynamically assign the variable using its name
        globals()[var] = get_content(var)

def evaluate_expression(expr):
    assign_variables(expr)
    return eval(expr)

# Example Usage
expr = 'delta(TMSRX_CLOSE, 5)'
result = evaluate_expression(expr)
print(result)

  
  
//// USING eval :

  # https://stackoverflow.com/questions/33646365/python-programming-how-to-eval-values-from-dictionary
# https://realpython.com/courses/evaluate-expressions-dynamically-python-eval/

from myfunctions import rank # from myfunctions import rank, mult

#def rank(n):
#    return n/(100.0-0.0)

def mult(n, m): # either define function in the same file OR in a separate file and import eg: see rank() function above
    return n*m

#expr = "dept_id == 'CS1' and student_count > 75 "
#d = {'dept_id': 'CS1' ,'student_count': 95 }
#eval(s, d)
#True
#>>> d = {'dept_id': 'CS1' ,'student_count': 35 }
#>>> eval(s, d)
#False

n=10
m=2
expr = "mult(rank(n), m)"
dict = {'rank': rank ,'mult': mult }

result = eval(expr)
#eval(expr, dict)
print(result)


===========//////////


//// =================

generate random expression :
  
# funtion: [type of function, type of output]
function_type = {'normalize':['scaled values','scaled'], 'neutralize':['standardize data','standard'], 'row_rank':['scaled values','scaled'],'ts_rank':['outlier reduction','ranked'],
         'ts_max':['basedata','float'], 'ts_arg_max':['date','date values'],'group_neut':['standardize data','standard'], 'group_rank':['outlier reduction','ranked'], 'bucket_rank': ['outlier reduction','ranked']}
# data: type of data
data_type = {'cp.head()':'basedata ,float','technical_indicator.head()': 'basedata ,float','sector.head()': 'basedata ,string'}

import random

functions = ['ts_rank','row_rank','normalize','group_rank', 'bucket_rank']
data = ['cp.head()','technical_indicator.head()']

# Number of expressions to generate
num_expressions = 20

# Generate random formula expressions and their types


def generate_expression(alpha_type):
    if random.random() < 0.8:  # 80% chance to generate a function
        function = random.choice(functions)
        inner_expression, alpha_type = generate_expression(alpha_type)
        alpha_type[0] = function_type[function][1]
        return f"{function}({inner_expression})" , alpha_type 
    else:
        data_placeholder = random.choice(data)
        alpha_type[1] = data_type[data_placeholder]
        return data_placeholder, alpha_type 

expressions = [generate_expression(['type','type']) for _ in range(num_expressions)]

# Print the generated expressions
for expression in expressions:
    print(expression)
  
  
=====================///
  
//==============

  quantstats : 

  qs.plots.snapshot(stock, title='PnL Performance', show=True)

 stock : 
<class 'pandas.core.series.Series'>


Date
2022-09-02   -0.00650
2022-09-06   -0.01258
2022-09-07    0.04119
2022-09-08    0.00484
2022-09-09    0.00319
Name: PnL, dtype: float64

stock.index = DatetimeIndex(['2010-01-04', '2010-01-05', '2010-01-06', '2010-01-07',
               '2010-01-08', '2010-01-11', '2010-01-12', '2010-01-13',
               '2010-01-14', '2010-01-15',
               ...
               '2022-08-26', '2022-08-29', '2022-08-30', '2022-08-31',
               '2022-09-01', '2022-09-02', '2022-09-06', '2022-09-07',
               '2022-09-08', '2022-09-09'],
              dtype='datetime64[ns]', name='Date', length=3168, freq=None)
============ //


  

  
  


  
def ts_arg_max(df, n_days):
    """
    Calculates the number of days since the maximum value in a rolling window of n_days for each column in a DataFrame.

    :param df: DataFrame containing data, with dates as rows and column identifiers.
    :param n_days: Number of days to consider for the rolling window.
    :return: DataFrame with the number of days since the maximum value in each rolling window.
    """
    def days_since_max(column):
        # Reverse the column for rolling to work from past to present
        reversed_col = column.iloc[::-1]
        # Rolling window applied
        return reversed_col.rolling(window=n_days, min_periods=1).apply(lambda x: x.argmax())

    return df.apply(days_since_max)

# Creating a sample DataFrame for testing the ts_arg_max function
sample_data_5 = {
    'Stock1': [10, 12, 14, 15, 14, 13, 12, 11, 10, 9],
    'Stock2': [20, 19, 18, 17, 18, 19, 20, 21, 22, 23],
    'Stock3': [30, 31, 30, 29, 28, 27, 28, 29, 30, 31]
}
sample_df_5 = pd.DataFrame(sample_data_5, index=pd.date_range(start='2021-05-01', periods=10, freq='D'))

# Applying the function to the sample DataFrame with n_days = 3
ts_arg_max_df = ts_arg_max(sample_df_5, n_days=3)
ts_arg_max_df
  
## SIMULATOR VISUALISATION  : https://github.com/ranaroussi/quantstats
#%pip install quantstats

import quantstats as qs

# extend pandas functionality with metrics, etc.
qs.extend_pandas()

# fetch the daily returns for a stock
#port = qs.utils.download_returns('META')
initial_capital = 1e6
stock = pnl['PnL']/initial_capital * 100.0 # stock is extended to be returns stream 

# show sharpe ratio

qs.plots.snapshot(stock, title='PnL Performance', show=True)

qs.reports.html(stock, "SPY")

print(stock.sharpe())
print(stock.max_drawdown())
print(stock.win_rate())
print(stock.avg_win())
print(stock.avg_loss())
print(stock.kelly_criterion())

GENERATE MOCK DATA :
  dates = pd.date_range('2020-01-01', '2020-01-10')
alpha_data = pd.DataFrame(np.random.randn(10, 3), index=dates, columns=['A', 'B', 'C'])
ret_data = pd.DataFrame(np.random.randn(10, 3)/100, index=dates, columns=['A', 'B', 'C'])  # returns in percentage

  ANOTHER WAY GENERATE MOCK DATA :
  # Set random seed for reproducibility
np.random.seed(42)
# Generate a DataFrame with 1000 rows (dates) and 20 columns (stocks)
n_dates = 1000
n_stocks = 50
# Generate random dates
date_rng = pd.date_range(start='1/1/2010', periods=n_dates, freq='D')
# Generate random percentage returns
random_returns = np.random.rand(n_dates, n_stocks) * 0.02 - 0.01
# Create stock names
stock_names = [f'Stock_{i+1}' for i in range(n_stocks)]
# Create the DataFrame
df_returns_random = pd.DataFrame(random_returns, index=date_rng, columns=stock_names)
  
  
## PLOT RANDOM X STOCKS :

  num_random_indices = 10
random_indices = np.random.choice(cp_clean.columns, num_random_indices)
print(random_indices)

# Iterate through the randomly generated column indices
for col_idx in random_indices:
    df_temp = pd.DataFrame()
    df_temp['Close'] = cp_clean.iloc[-100:,int(col_idx)]
    df_temp['Max_fall'] = max_fall_from_last_high(df_temp['Close'])
    #print(df_temp)
    plot_dual_axis(df_temp['Close'], df_temp['Max_fall'])


def winsorize(data, z):
    """
    Clips values in a DataFrame or Series to within 'z' standard deviations of the mean.

    :param data: DataFrame or Series to be winsorized.
    :param z: Number of standard deviations to use for clipping.
    :return: Winsorized DataFrame or Series.
    """
    if isinstance(data, pd.DataFrame):
        # Calculate mean and standard deviation for each column
        mean = data.mean()
        std = data.std()

        # Define clipping bounds
        lower_bound = mean - z * std
        upper_bound = mean + z * std

        # Apply clipping
        winsorized_data = data.clip(lower=lower_bound, upper=upper_bound, axis=1)

    elif isinstance(data, pd.Series):
        # Calculate mean and standard deviation
        mean = data.mean()
        std = data.std()

        # Define clipping bounds
        lower_bound = mean - z * std
        upper_bound = mean + z * std

        # Apply clipping
        winsorized_data = data.clip(lower=lower_bound, upper=upper_bound)

    else:
        raise TypeError("Input data must be a pandas DataFrame or Series")

    return winsorized_data

# Example usage:
# For a DataFrame 'df'
# winsorized_df = winsorize(df, z=2)

# For a Series 'series'
# winsorized_series = winsorize(series, z=2)

def neutralize(df, group, include_itself=True, min_group_size=2):
    """
    Neutralizes each value in 'df' by subtracting the mean of its respective group as defined in 'group'.
    Groups with fewer than min_group_size stocks are excluded from the neutralization process.

    :param df: DataFrame containing stock prices, with dates as rows and stocks as columns.
    :param group: Series containing group IDs for each stock.
    :param include_itself: Boolean indicating whether to include the stock's own value in the mean calculation.
    :param min_group_size: Minimum number of stocks in a group to be included in neutralization.
    :return: Neutralized DataFrame.
    """
    # Ensure the group series is aligned with the columns of df
    group = group.reindex(df.columns)

    # Count the number of stocks in each group
    group_counts = df.groupby(group, axis=1).count()

    # Calculate group means with the specified method
    if include_itself:
        group_means = df.groupby(group, axis=1).transform('mean')
    else:
        group_means = (df.groupby(group, axis=1).transform('sum') - df) / (df.groupby(group, axis=1).transform('count') - 1)

    # Apply neutralization only to groups with a sufficient number of stocks
    neutralized_df = df.sub(group_means).where(group_counts >= min_group_size, df)

    return neutralized_df

# Example usage:
# neutralized_df = neutralize(df, group, include_itself=True, min_group_size=2)
  
  # ALSO set all exact 0 to NAN,
def neutralize(df, group, include_itself=True):
    """
    Neutralizes each value in 'df' by subtracting the mean of its respective group as defined in 'group'.
    If include_itself is True, includes the stock's own value in the mean calculation. If False, excludes it.

    :param df: DataFrame containing stock prices, with dates as rows and stocks as columns.
    :param group: Series containing group IDs for each stock.
    :param include_itself: Boolean indicating whether to include the stock's own value in the mean calculation.
    :return: Neutralized DataFrame.
    """
    # Ensure the group series is aligned with the columns of df
    group = group.reindex(df.columns)

    if include_itself:
        # Calculate group means for each date including the stock itself
        group_means = df.groupby(group, axis=1).transform('mean')
    else:
        # Calculate group means for each date excluding the stock itself
        group_means = (df.groupby(group, axis=1).transform('sum') - df) / (df.groupby(group, axis=1).transform('count') - 1)

    # Subtract the group mean from each stock's value
    neutralized_df = df.sub(group_means)

    return neutralized_df

# Example usage:
# Assuming df is your DataFrame of stock prices and group is the Series of group IDs
# neutralized_df_included = neutralize(df, group, include_itself=True)
# neutralized_df_excluded = neutralize(df, group, include_itself=False)
# print(neutralized_df_included)
# print(neutralized_df_excluded)


## ANALYZING TURNOVER :
  ## ANALYZING ALPHA : TURNOVER

from scipy.stats import linregress

def calculate_turnover(alpha):
    alpha_df = pd.DataFrame(alpha)
    turnover = alpha_df.diff().abs().sum(axis=1) / 2
    return turnover

alpha = np.random.rand(*ret1_d1.shape)
alpha = alpha / alpha.sum(axis=1)[:, np.newaxis]

returns_alpha = np.sum(alpha * ret1_d1.fillna(0).values, axis=1)
returns_series = pd.Series(returns_alpha)

## ANALYZING TURNOVER :
# 1. Correlate Turnover with Returns: Check if higher turnover periods are associated with higher returns. If not, the high turnover might just be noise. In this case, can use smoothing or regularization or longer holding periods or batch trading(aggregate trades first)
# 2. Transaction Costs: High turnover means higher transaction costs. Make sure that the extra returns you're getting are not being eaten up by these costs.
# 3. Consistency: If the alpha is consistently profitable but has high turnover, you might be able to reduce the turnover without sacrificing profitability by applying some form of regularization or smoothing to your alpha construction logic.
# 4. Review Alpha Factors: Carefully review the alpha factors you're using. Are they all contributing positively to your returns? Removing unnecessary factors can reduce turnover. 

turnover = calculate_turnover(alpha)
correlation_with_returns = turnover.corr(returns_series)


## FUNCTIONS 

  def ts_max(df, window):
    df = df.rolling(window).max()
    return df
    
def ts_arg_max(df, window):
    ## this function calculates the index of the max value in the rolling window, this index is the actual index in the dataframe
    def idx_max(x):
        dict_values = [x.shift(i) for i in range(window)]
        df_shift = pd.DataFrame(dict(zip(range(window),dict_values))).fillna(0)
        return x.index[np.arange(len(x)) - df_shift.idxmax(1)]
    return df.apply(lambda x: idx_max(x))
    
def group_rank(df, group):
    ## implement this function, this function will apply a custom function, 'rank' in this case(.apply() above rank function) , group-wise where group is sector here. So instead of applying rank to whole row, it will only rank the stock among it's sector stocks
    df_merge = pd.merge(df.T, group.replace(np.nan,'NAN').T, right_index = True, left_index = True)
    df = df.apply(lambda x: df_merge.groupby(x.name+'_y',)[x.name+'_x'].rank(), axis = 1)
    return df
    
def bucket_rank(df, n_bucket):
    ## implement this function , 
    ## this takes in a df_bucket dataframe
    ##    For the df_bucket, first convert each row to uniform values between [0,1] using min-max scaling ie (value - min(value across the row))/(max-min)
    ##    Now divide the ranged bucket values to n_bucket, eg : if n_bucket=5, then df_bucket=0-0.2 will be bucket1, 0.2-0.4 will be bucket2 and so on...
    ##    Now we have bucket number for each stock for each dasy just like sector, this is the new classification
    
    ## Now apply rank to this new group 'bucket' , apply rank like a customized functionreturn because I would like to change it to other functions like neutralizeation etc as well
    ## Make the code easy to understand and extendible by me
    
    groups = [f'bucket{i}' for i in range(1, n_bucket+1)]
    edges = np.linspace(0,1,num = n_bucket+1)
    df_group = df.apply(lambda row: pd.cut((row - row.min())/(row.max()-row.min()), bins = edges, labels = groups, duplicates='drop').astype('O')) 
    return group_rank(df,df_group)

  

  
  # eutralizes group which is a series adn df is proper
def neutralize(df, group):
    """
    Neutralizes each value in 'df' by subtracting the mean of its respective group as defined in 'group'.

    :param df: DataFrame containing stock prices, with dates as rows and stocks as columns.
    :param group: Series containing group IDs for each stock.
    :return: Neutralized DataFrame.
    """
    # Ensure the group series is aligned with the columns of df
    group = group.reindex(df.columns)

    # Calculate group means for each date
    group_means = df.groupby(group, axis=1).transform('mean')

    # Subtract the group mean from each stock's value
    neutralized_df = df.sub(group_means)

    return neutralized_df

  
def truncate_new(df, fraction):
    """
    Truncates each value in a DataFrame based on a specified fraction of the absolute sum of values in that row.

    :param df: DataFrame to be truncated.
    :param fraction: The fraction of the total absolute sum to limit each value to.
    :return: Truncated DataFrame.
    """
    # Calculate the total absolute sum per row
    total_abs_sum = df.abs().sum(axis=1)

    # Apply conditional truncation
    truncated_df = df.apply(lambda row: np.where(row > fraction * total_abs_sum[row.name], 
                                                 fraction * total_abs_sum[row.name], 
                                                 np.where(row < -fraction * total_abs_sum[row.name], 
                                                          -fraction * total_abs_sum[row.name], 
                                                          row)), axis=1)

    return pd.DataFrame(truncated_df.tolist(), index=df.index, columns=df.columns)

# Applying the new truncate function to the example DataFrame
truncated_df_new = truncate_new(df_example, 0.02)
truncated_df_new


  
def group_mean(df, group, include_itself=True):
    """
    Calculates the mean of all values in the group of a particular stock.

    :param df: DataFrame containing stock prices, with dates as rows and stocks as columns.
    :param group: DataFrame containing industry IDs for each stock, with same structure as df.
    :param include_itself: If True, include the stock's own value in mean calculation. If False, exclude it.
    :return: DataFrame of group means for each stock.
    """
    # Check if df and group have the same structure
    if df.shape != group.shape or (df.index != group.index).any() or (df.columns != group.columns).any():
        raise ValueError("df and group must have the same shape, indices, and columns")

    # Create a copy of df for modification
    group_means = df.copy()

    # Iterate over each date and stock
    for date in df.index:
        for stock in df.columns:
            group_id = group.at[date, stock]
            group_values = df.loc[date][group.loc[date] == group_id]

            if not include_itself:
                group_values = group_values[group_values.index != stock]

            group_mean_value = group_values.mean()
            group_means.at[date, stock] = group_mean_value

    return group_means

# Testing the group_mean function with the same example data
group_means_included = group_mean(df_example, group_example, include_itself=True)
group_means_excluded = group_mean(df_example, group_example, include_itself=False)

group_means_included, group_means_excluded


def neutralize(df, group, include_itself=True):
    """
    Neutralizes each value in 'df' by subtracting the mean of all stocks in the same 'group'.

    :param df: DataFrame containing stock prices, with dates as rows and stocks as columns.
    :param group: DataFrame containing industry IDs for each stock, with same structure as df.
    :param include_itself: If True, include the stock's own value in mean calculation. If False, exclude it.
    :return: Neutralized DataFrame.
    """
    # First, calculate the group means
    group_means = group_mean(df, group, include_itself)

    # Neutralize by subtracting the group mean from each stock's value
    neutralized_df = df.sub(group_means)

    return neutralized_df

# Testing the neutralize function with the same example data
neutralized_df_included = neutralize(df_example, group_example, include_itself=True)
neutralized_df_excluded = neutralize(df_example, group_example, include_itself=False)

neutralized_df_included, neutralized_df_excluded

def neutralize(df, group):
    """
    Neutralizes each value in 'df' by subtracting the mean of all stocks in the same 'group'.

    :param df: DataFrame containing stock prices, with dates as rows and stocks as columns.
    :param group: DataFrame containing industry IDs for each stock, with same structure as df.
    :return: Neutralized DataFrame.
    """
    # Ensuring the indices and columns of df and group match
    if df.shape != group.shape or (df.index != group.index).any() or (df.columns != group.columns).any():
        raise ValueError("df and group must have the same shape, indices, and columns")

    # Applying the neutralization
    return df.sub(df.groupby(group).transform('mean'))
  
def find_missing_values_NAN(df):

    '''
        This function finds out missing values (as NAN) in between valid non-NAN values
    '''
    
    print("Missing values as NAN : \n")
    
    found=False
    for i in range(1, len(df) - 1):  # Exclude first and last row
        for j in range(1, len(df.columns) - 1):  # Exclude first and last column
            if pd.isna(df.iloc[i, j]) and not pd.isna(df.iloc[i-1, j]) and not pd.isna(df.iloc[i+1, j]):
                print(f"NaN value surrounded by non-NaN values at (row {i}, column {j})\n")
                print(df.iloc[i-2:i+2,j])
                found=True
                break
            if found:
                break
                
def find_missing_values_zero(df):
    
    '''
        This function finds out missing values (as 0) in between valid non-NAN values
    '''
    
    print("Missing values as 0 : \n")
    
    found=False
    for i in range(1, len(df) - 1):  # Exclude first and last row
        for j in range(1, len(df.columns) - 1):  # Exclude first and last column
            if df.iloc[i, j] == 0 and df.iloc[i-1, j] != 0 and df.iloc[i+1, j] != 0:
                print(f"zero value surrounded by non-zero values at (row {i}, column {j})\n")
                print(df.iloc[i-2:i+2,j])
                found=True
                break
            if found:
                break

def find_inf_values(df):
    
    '''
        This function find inf values in the data
    '''
    
    print("Check for inf values : \n")
    
    found=False
    for i in range(1, len(df) - 1):  # Exclude first and last row
        for j in range(1, len(df.columns) - 1):  # Exclude first and last column
            if np.isinf(df.iloc[i, j]) and not np.isinf(df.iloc[i-1, j]) and not np.isinf(df.iloc[i+1, j]):
                print(f"inf value surrounded by non-inf values at (row {i}, column {j})\n")
                print(df.iloc[i-2:i+2,j])
                found=True
                break
            if found:
                break
                
def find_non_NAN_rows(df):
    non_nan_rows = df[df.apply(lambda row: row.notna().any(), axis=1)]
    print(non_nan_rows.head(10))

def plot_histogram_whole_data(df, range_boundary):

    fig, ax = plt.subplots()  # Create a new figure and axes for each plot
    
    
    df = df.stack()
    df = df.reset_index(drop=True)
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.dropna(axis=0, how='any', inplace=True)
    #splt.hist(df,bins=20)
    #plt.hist(df, bins=20, range=(-1*range_boundary, range_boundary))
    
    ax.hist(df, bins=20, range=(-1 * range_boundary, range_boundary))
    
# cross-section plotting
def plot_cs(df, row_index, x_title, bins=20):
    
    fig, ax = plt.subplots()  # Create a new figure and axes for each plot
    
    df = df.iloc[row_index, :]
    #plt.hist(df, bins=20)  
    ax.hist(df, bins=20)
    plt.xlabel(x_title)
    plt.ylabel('Frequency')
    plt.title('Cross-Sectional Plot : Histogram of '+str(x_title))
    plt.show();

# time series plottimg
def plot_ts(df, column_index, y_title):

    
    #if remove_nan:
    #    df = df.iloc[:,column_index].dropna()
   
    plt.plot(df.index, df.iloc[:, column_index])
    plt.xlabel('Date')
    plt.ylabel(y_title)
    plt.title('Time Series Plot : '+str(y_title)+' for Column {}'.format(column_index))
    plt.show();

def plot_ts_df1_vs_df2(df1, df2, column_index):
    """
    Plot time series for two given dataframes for a specified column.
    
    Parameters:
    df1 (DataFrame): DataFrame containing first set of data with dates as index.
    df2 (DataFrame): DataFrame containing second set of data with dates as index.
    column_index (int or str): Column representing the stock in both DataFrames.
    
    Returns:
    None: Displays the plot.
    """
    
    # Convert column_index to string to match DataFrame's column type
    column_index = str(column_index)
    
    # Get DataFrame names for labels
    df1_name = df1.name if hasattr(df1, 'name') else 'df1'
    df2_name = df2.name if hasattr(df2, 'name') else 'df2'
    
    fig, ax1 = plt.subplots(figsize=(15, 8))
    
    # Plot data from df1
    ax1.set_xlabel('Date')
    ax1.set_ylabel(df1_name, color='tab:blue')
    ax1.plot(df1.index, df1[column_index], color='tab:blue', label=df1_name)
    ax1.tick_params(axis='y', labelcolor='tab:blue')
    
    # Create a secondary y-axis to plot data from df2
    ax2 = ax1.twinx()
    ax2.set_ylabel(df2_name, color='tab:red')
    ax2.plot(df2.index, df2[column_index], color='tab:red', label=df2_name)
    ax2.tick_params(axis='y', labelcolor='tab:red')
    
    # Show legend and grid
    ax1.legend(loc='upper left')
    ax2.legend(loc='upper right')
    ax1.grid(True)
    
    plt.title(f'Time Series for {df1_name} and {df2_name} for Column {column_index}')
    plt.show()

    
    
def plot_ts_random(df, n=4):
    """
    Plot n random time series columns from a given DataFrame df as subplots in a 2x(n/2) table format.
    
    Parameters:
    df (DataFrame): DataFrame containing the time series data.
    n (int): Number of random columns to plot. Default is 4, should be an even number.
    
    Returns:
    None: Displays the subplots.
    """
    
    # Ensure n is an even number for 2x(n/2) table format
    if n % 2 != 0:
        n = int(n/2)*2
    
    # Get DataFrame name for labeling purposes
    df_name = df.name if hasattr(df, 'name') else 'DataFrame'
    
    # Select n random columns from the DataFrame
    random_columns = random.sample(list(df.columns), n)
    
    # Create subplots in 2x(n/2) table format
    fig, axs = plt.subplots(int(n/2), 2, figsize=(15, 4 * int(n/2)))
    
    for i, column in enumerate(random_columns):
        row = i // 2
        col = i % 2
        axs[row, col].plot(df.index, df[column])
        axs[row, col].set_xlabel('Date')
        axs[row, col].set_ylabel(df_name)
        axs[row, col].set_title(f'Column {column}')
    
    plt.tight_layout()
    plt.show()
    
'''
# display histogram of % of non-NAN values for each column
def plot_data_completeness(df):
    
    non_nan_percentage = df.notna().mean() * 100
    plt.figure(figsize=(10, 6))
    
    plt.hist(non_nan_percentage, bins=50, edgecolor='black')
    plt.title('Histogram of Percentage of Non-NaN Entries Per Column')
    plt.xlabel('Percentage of Non-NaN Entries')
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.show()
    return non_nan_percentage
'''

# display histogram of % of non-NAN values for each column
# trim_leading_NANs True means you compute the calculation after removing leading NANs
def plot_data_completeness(df, trim_leading_NANs=False):
    if trim_leading_NANs:
        # Initialize an empty dictionary to store the non-NaN percentages after trimming leading NaNs
        non_nan_percentage = {}
        for col in df.columns:
            # Drop leading NaNs
            trimmed_series = df[col].dropna()
            if len(trimmed_series) > 0:
                first_valid_index = df[col].first_valid_index()
                trimmed_series = df.loc[first_valid_index:, col]
                non_nan_percent = (trimmed_series.count() / len(trimmed_series)) * 100
            else:
                non_nan_percent = 0.0
            non_nan_percentage[col] = non_nan_percent
        non_nan_percentage = pd.Series(non_nan_percentage)
    else:
        non_nan_percentage = df.notna().mean() * 100
    
    plt.figure(figsize=(10, 6))
    plt.hist(non_nan_percentage, bins=50, edgecolor='black')
    plt.title('Histogram of Percentage of Non-NaN Entries Per Column')
    plt.xlabel('Percentage of Non-NaN Entries')
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.show()
    
    return non_nan_percentage

def normalize(df):
    df = df.sub(df.mean(axis=1), axis=0)
    df = df.div(df.abs().sum(axis=1), axis=0)
    return df
    
def neutralize(df):
    df = df.sub(df.mean(axis=1), axis=0)
    return df

##def neutralized_with_sector(df):
    ### IMPLEMENT? ?
    
def rank(df):
    df = df.apply(lambda row: row.rank(), axis=1)
    df = df.div(df.sum(axis=1), axis=0)
    return df

def ts_rank(df, n):
    #df = df.squeeze()
    # rolling applies to series
    df = df.rolling(window=n).apply(rank)
      
def group_mean(df, group):

    '''
    Returns data with average value of returns based on sector for each day and stock
    '''
    # The function uses 2 dictionaries; 1st to get sectors from sector data as keys and mean of returns for each sector for a day as values
    # 2nd dict contains date as keys and 1st dictionary as value for correcsponding date
    # here function uses Transpose of data to convert dates in columns 
    # as a last step function replaces sectors in sector data with mean values for each column and again applies Transpose to get original form

    keys = []
    values = []
    for col in group.T.columns:
        daywise_df = pd.DataFrame({'sector':group.T[col],'returns':df.T[col]}) # data of sector info and stocks' returns on particular day
        dict_sector_mean = dict(daywise_df.groupby('sector')['returns'].mean()) # dict of sector with corresponding mean value for a particular day
        values.append(dict_sector_mean)
        keys.append(col)
    return group.T.replace(dict(zip(keys,values))).T # Replace sector with corresponding means
    
def neutralize_to_group(df, group):
    df = df.sub(group_mean(df, group))
    return df

def replace_outliers_with_nan(df):
    '''
        This function replaces values which are higher than 99th percentile or below than 1 percentile by NAN
    '''
    q99 = df.quantile(0.99)
    q1 = df.quantile(0.01)
    
    df[df > q99] = np.nan
    df[df < q1] = np.nan
    
    return df

def calculate_cvar(returns, alpha=0.05):
    """
    Calculate the Conditional Value-at-Risk (CVaR) of a series of returns.
    
    Parameters:
    - returns (numpy.array): Array of returns
    - alpha (float): Significance level, default is 0.05
    
    Returns:
    - float: CVaR value
    """
    # Sort the returns
    sorted_returns = np.sort(returns)
    
    # Calculate the index corresponding to the alpha quantile
    index = int(alpha * len(sorted_returns))
    
    # Calculate CVaR
    cvar = np.mean(sorted_returns[:index])
    
    return cvar

Kalman filter smoothening :

def kalman_filter_smoothing_df(df, observation_covariance, transition_covariance):
    n_rows, n_cols = df.shape
    smoothed_df = pd.DataFrame(index=df.index, columns=df.columns)

    for col in df.columns:
        series = df[col]
        initial_state = series.dropna().iloc[0] if not series.dropna().empty else 0
        state = initial_state
        state_covariance = 1.0
        filtered_state_means = np.zeros(n_rows)

        for t in range(n_rows):
            # Prediction Step
            predicted_state = state
            predicted_state_covariance = state_covariance + transition_covariance

            # Update Step
            observation = series.iloc[t] if not np.isnan(series.iloc[t]) else predicted_state  # Use prediction if observation is NaN
            kalman_gain = predicted_state_covariance / (predicted_state_covariance + observation_covariance)
            state = predicted_state + kalman_gain * (observation - predicted_state)
            state_covariance = (1 - kalman_gain) * predicted_state_covariance

            filtered_state_means[t] = state

        smoothed_df[col] = filtered_state_means

    return smoothed_df

# Testing the adapted function on the first two columns of the data
observation_covariance_test = 0.5
transition_covariance_test = 0.05
smoothed_data_test = kalman_filter_smoothing_df(cp.iloc[:, :7], observation_covariance_test, transition_covariance_test)
print(smoothed_data_test.head(5))

# plot 7th column
plt.figure(figsize=(14, 6))
plt.plot(cp.iloc[:,6], label=f'Original', color='blue', linewidth=2)
plt.plot(smoothed_data_test.iloc[:,6], label=f'Kalman Smoothed (ObsCov={observation_covariance}, TransCov={transition_covariance})', color='green', linewidth=2)            
plt.title(f'Stock Price: Kalman Filter vs Original')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

def plot_series_histogram(series):
    """
    Plots the histogram of a pandas Series.

    :param series: Pandas Series to be plotted.
    """
    # Plotting the histogram
    plt.hist(series, bins='auto', color='blue', alpha=0.7)
    plt.title('Histogram of Series')
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    plt.show()
-->

</body>
</html>
